{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Mamiglia/challenge.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport h5py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nfrom huggingface_hub import hf_hub_download\nfrom transformers import CLIPModel, CLIPProcessor\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nimport math\nimport pickle\n\nfrom transformers import CLIPModel,CLIPProcessor\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers.utils.hub as hub_utils \nfrom PIL import Image\nimport pandas as pd\nfrom collections import defaultdict\nfrom challenge.src.eval import evaluate_retrieval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SECTION 1: DOWNLOAD MANAGER\n# ============================================================================\ndef get_dataset_paths():\n    \"\"\"Download the 3 critical components from HuggingFace\"\"\"\n    print(\"Downloading files from HuggingFace...\")\n    repo = \"pscotti/mindeyev2\"\n    \n    betas_path = hf_hub_download(\n        repo_id=repo, \n        filename=\"betas_all_subj01_fp32_renorm.hdf5\", \n        repo_type=\"dataset\"\n    )\n    \n    images_path = hf_hub_download(\n        repo_id=repo, \n        filename=\"coco_images_224_float16.hdf5\", \n        repo_type=\"dataset\"\n    )\n    \n    behav_path = hf_hub_download(\n        repo_id=repo, \n        filename=\"COCO_73k_subj_indices.hdf5\", \n        repo_type=\"dataset\"\n    )\n    \n    print(\"✓ Files downloaded successfully\")\n    return betas_path, images_path, behav_path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MindEyeDataset(Dataset):\n    def __init__(self, betas_file, images_file, behav_file, \n                 subject='subj01', transform=None):\n        \"\"\"\n        Args:\n            betas_file: Path to fMRI betas HDF5\n            images_file: Path to images HDF5\n            behav_file: Path to behavior/indices HDF5\n            subject: Subject ID (default: 'subj01')\n            transform: Optional image transforms (for CLIP preprocessing)\n        \"\"\"\n        # Store paths instead of file handles (for multiprocessing)\n        self.betas_path = betas_file\n        self.images_path = images_file\n        self.transform = transform\n        \n        # Load metadata (image indices for each trial)\n        with h5py.File(behav_file, 'r') as f:\n            self.behav_indices = f[subject][:]\n        \n        # File handles will be opened per-worker in __getitem__\n        self._betas_file = None\n        self._images_file = None\n        \n        print(f\"Dataset initialized: {len(self.behav_indices)} trials\")\n    \n    def _ensure_files_open(self):\n        \"\"\"Lazy open files (called in __getitem__)\"\"\"\n        if self._betas_file is None:\n            self._betas_file = h5py.File(self.betas_path, 'r')\n            self._images_file = h5py.File(self.images_path, 'r')\n    \n    def __len__(self):\n        return len(self.behav_indices)\n    \n    def __getitem__(self, idx):\n        # Ensure files are open (lazy initialization per worker)\n        self._ensure_files_open()\n        \n        # OPTIMIZED: Direct access to open files (very fast!)\n        fmri_data = self._betas_file['betas'][idx]\n        \n        # Get image ID (direct mapping: behav_indices = cocoidx)\n        image_id = self.behav_indices[idx]\n        \n        # Load image using cocoidx\n        image_data = self._images_file['images'][image_id]\n        \n        # Convert to tensors\n        fmri_tensor = torch.tensor(fmri_data, dtype=torch.float32)\n        \n        \n        image_tensor = torch.tensor(image_data, dtype=torch.float32)\n        \n        # Apply transforms if provided (for CLIP)\n        if self.transform:\n            image_tensor = self.transform(image_tensor)\n        \n        return {\n            'fmri': fmri_tensor,      # (15724,)\n            'image': image_tensor,     # (3, 224, 224) in [0, 1]\n            'image_id': image_id,      # COCO image ID (0-72999)\n            'trial_idx': idx           # Trial index (0-29999 for subj01)\n        }\n    \n    def __del__(self):\n        \"\"\"Close HDF5 files when dataset is destroyed\"\"\"\n        if self._betas_file is not None:\n            self._betas_file.close()\n        if self._images_file is not None:\n            self._images_file.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SECTION 3: TRAIN/VAL SPLIT (NO DATA LEAKAGE)\n# ============================================================================\ndef create_splits(dataset, train_ratio=0.8, seed=42):\n    \"\"\"\n    Split by IMAGE ID to avoid data leakage from repetitions\n    \n    Returns:\n        train_indices, val_indices: Lists of trial indices\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Group trials by image_id\n    image_to_trials = defaultdict(list)\n    for idx in range(len(dataset)):\n        image_id = dataset.behav_indices[idx]\n        image_to_trials[image_id].append(idx)\n    \n    # Split at IMAGE level (not trial level!)\n    unique_images = list(image_to_trials.keys())\n    np.random.shuffle(unique_images)\n    \n    n_train_images = int(len(unique_images) * train_ratio)\n    train_images = set(unique_images[:n_train_images])\n    val_images = set(unique_images[n_train_images:])\n    \n    # Collect trial indices\n    train_indices = []\n    val_indices = []\n    \n    for image_id, trial_list in image_to_trials.items():\n        if image_id in train_images:\n            train_indices.extend(trial_list)\n        else:\n            val_indices.extend(trial_list)\n    \n    print(f\"\\n=== Dataset Split ===\")\n    print(f\"Train: {len(train_images)} images, {len(train_indices)} trials\")\n    print(f\"Val:   {len(val_images)} images, {len(val_indices)} trials\")\n    print(f\"Repetitions per image: ~{len(train_indices)/len(train_images):.1f}\")\n    \n    return train_indices, val_indices\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SECTION 4: CLIP EMBEDDINGS EXTRACTOR\n# ============================================================================\nclass CLIPEmbeddingsExtractor:\n    def __init__(self, model_name=\"openai/clip-vit-large-patch14\", device='cuda'):\n        \"\"\"\n        Load CLIP model for extracting image embeddings\n        \n        Args:\n            model_name: CLIP model variant\n            device: 'cuda' or 'cpu'\n        \"\"\"\n        print(f\"\\nLoading CLIP model: {model_name}\")\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        \n        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.model.eval()\n        \n        print(f\"✓ CLIP loaded on {self.device}\")\n        print(f\"  Embedding dimension: {self.model.config.projection_dim}\")\n    \n    @torch.no_grad()\n    def extract_image_embeddings(self, images):\n        \"\"\"\n        Extract CLIP embeddings from images\n        \n        Args:\n            images: Tensor of shape (B, 3, 224, 224) in range [0, 1]\n        \n        Returns:\n            embeddings: Tensor of shape (B, 768) for ViT-Large\n        \"\"\"\n        # CLIP expects images in [0, 1] range (already normalized in dataset)\n        # But we need to apply CLIP's specific normalization\n        \n        # Move to correct device\n        images = images.to(self.device)\n        \n        # Get image features (not normalized)\n        image_features = self.model.get_image_features(pixel_values=images)\n        \n        # Normalize embeddings (CLIP does this internally for similarity)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        \n        return image_features\n    \n    def preprocess_images(self, images):\n        \"\"\"\n        Apply CLIP preprocessing to raw images\n        \n        Args:\n            images: Tensor (B, 3, 224, 224) in [0, 1]\n        \n        Returns:\n            Preprocessed tensor ready for CLIP\n        \"\"\"\n        # Convert to PIL for processor\n        # Note: For simplicity, we'll use the raw tensors\n        # CLIP processor normalizes with ImageNet stats\n        \n        # Manual normalization (CLIP uses ImageNet stats)\n        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1)\n        std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1)\n        \n        images = (images - mean.to(images.device)) / std.to(images.device)\n        \n        return images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SECTION 5: CLIP EMBEDDING CACHE BUILDER\n# ============================================================================\ndef build_clip_cache(dataset, clip_extractor, cache_path='clip_embeddings.pt', \n                     batch_size=64):\n    \"\"\"\n    Pre-compute all CLIP embeddings and save to disk\n    This saves time during training!\n    \n    Args:\n        dataset: MindEyeDataset instance\n        clip_extractor: CLIPEmbeddingsExtractor instance\n        cache_path: Where to save embeddings\n        batch_size: Batch size for processing\n    \n    Returns:\n        embeddings: Tensor of shape (n_trials, 768)\n    \"\"\"\n    if os.path.exists(cache_path):\n        print(f\"Loading cached CLIP embeddings from {cache_path}\")\n        return torch.load(cache_path)\n    \n    print(f\"\\nBuilding CLIP embeddings cache...\")\n    print(f\"This will take a few minutes but only needs to be done once!\")\n    \n    dataloader = DataLoader(dataset, batch_size=batch_size, \n                           shuffle=False, num_workers=0)\n    \n    all_embeddings = []\n    \n    for batch in tqdm(dataloader, desc=\"Extracting CLIP embeddings\"):\n        images = batch['image']\n        \n        # Preprocess for CLIP\n        images = clip_extractor.preprocess_images(images)\n        \n        # Extract embeddings\n        embeddings = clip_extractor.extract_image_embeddings(images)\n        all_embeddings.append(embeddings.cpu())\n    \n    # Concatenate all embeddings\n    all_embeddings = torch.cat(all_embeddings, dim=0)\n    \n    # Save to disk\n    torch.save(all_embeddings, cache_path)\n    print(f\"✓ Saved {all_embeddings.shape[0]} embeddings to {cache_path}\")\n    print(f\"  Shape: {all_embeddings.shape}\")\n    \n    return all_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SECTION 6: DATASET WITH PRECOMPUTED CLIP EMBEDDINGS\n# ============================================================================\nclass MindEyeWithCLIP(Dataset):\n    \"\"\"\n    Dataset that returns (fMRI, CLIP_embedding) pairs\n    Much faster than computing CLIP on-the-fly!\n    \"\"\"\n    def __init__(self, base_dataset, clip_embeddings):\n        self.base_dataset = base_dataset\n        self.clip_embeddings = clip_embeddings\n    \n    def __len__(self):\n        return len(self.base_dataset)\n    \n    def __getitem__(self, idx):\n        fmri = self.base_dataset[idx]['fmri']\n        clip_emb = self.clip_embeddings[idx]\n        \n        return (\n            fmri,              # (15724,)\n            clip_emb, # (768,)\n            self.base_dataset[idx]['image_id']\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def symmetric_contrastive_loss(text_proj, image_emb, temperature=0.05):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    logits = torch.matmul(text_proj, image_emb.T) / temperature\n    batch_size = logits.shape[0]\n    labels = torch.arange(batch_size, device=logits.device)\n    loss_t2i = F.cross_entropy(logits, labels)\n    loss_i2t = F.cross_entropy(logits.T, labels)\n    return (loss_t2i + loss_i2t) / 2\n\ndef cosine_regression_loss(text_proj, image_emb):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    # Maximize diagonal similarities\n    cos_sim = (text_proj * image_emb).sum(dim=-1)\n    return (1 - cos_sim).mean()\n    \ndef triplet_loss(text_proj, image_emb, margin=0.4):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    batch_size = text_proj.shape[0]\n    sims = torch.matmul(text_proj, image_emb.T)\n    pos_sims = sims.diagonal()\n    mask = 1.0 - torch.eye(batch_size, device=sims.device)\n    neg_sims = sims * mask + torch.eye(batch_size, device=sims.device) * -1e9\n    hard_neg_sims, _ = neg_sims.max(dim=1)\n    loss = F.relu(margin - pos_sims + hard_neg_sims).mean()\n    return loss\n\n\ndef combined_loss(text_proj, image_emb, loss_arg):\n    alpha = loss_arg[\"ALPHA\"]\n    beta = loss_arg.get(\"BETA\", 0.3)  # New weight\n       \n    contrastive = symmetric_contrastive_loss(text_proj, image_emb, \n                                                loss_arg[\"TEMPERATURE\"])\n    triplet = triplet_loss(text_proj, image_emb, \n                             margin=loss_arg[\"MARGIN\"])\n    regression = cosine_regression_loss(text_proj, image_emb)\n       \n    return alpha * contrastive + beta * triplet + (1-alpha-beta) * regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_mrr_at_k_batched(text_proj, image_emb, k=100, batch_size=128):\n    text_proj = F.normalize(text_proj, dim=-1)\n    image_emb = F.normalize(image_emb, dim=-1)\n    \n    N = text_proj.shape[0]\n    reciprocal_ranks = []\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            batch_t = text_proj[start:end]\n            \n            sims = torch.matmul(batch_t, image_emb.T)\n            top_k_values, top_k_indices = torch.topk(sims, k=min(k, N), dim=1)\n            \n            for i in range(end - start):\n                true_idx = start + i\n                top_k_for_query = top_k_indices[i].cpu().numpy()\n                position = (top_k_for_query == true_idx).nonzero()\n                \n                if len(position[0]) > 0:\n                    rank = position[0][0] + 1\n                    reciprocal_ranks.append(1.0 / rank)\n                else:\n                    reciprocal_ranks.append(0.0)\n    \n    return sum(reciprocal_ranks) / len(reciprocal_ranks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(augmentation_par,train_par, loss_par,model, train_loader, val_loader, device, model_path,augmenter=None):\n    LR =train_par[\"LR\"]\n    WEIGHT_DEC = train_par[\"WEIGHT_DEC\"]\n    WARMUP = train_par[\"WARMUP\"]\n    USE_AUGMENTATION = augmentation_par[\"USE_AUGMENTATION\"]\n    epochs = train_par[\"EPOCHS\"]\n    \n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DEC)\n    warmup_epochs = WARMUP\n    scheduler_warmup = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)\n    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs-warmup_epochs, eta_min=1e-7)\n    \n    best_mrr = 0.0\n    patience_counter = 0\n    patience = 5\n    loss_fn = loss_par[\"FUNC\"]\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, y_batch, imaged_id in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            \n            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = loss_fn(outputs, y_batch,loss_par[\"ARG\"])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n    \n        train_loss /= len(train_loader)\n    \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, y_batch, imaged_id in val_loader:\n                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n                outputs = model(X_batch)\n                loss =  loss_fn(outputs, y_batch,loss_par[\"ARG\"])\n                val_loss += loss.item()\n    \n        val_loss /= len(val_loader)\n    \n        if (epoch + 1) % 2 == 0 or epoch == epochs - 1:\n            all_preds = []\n            all_targets = []\n            model.eval()\n            with torch.no_grad():\n                for X_batch, y_batch, imaged_id in val_loader:\n                    X_batch = X_batch.to(DEVICE)\n                    y_batch = y_batch.to(DEVICE)\n                    pred_batch = model(X_batch)\n                    all_preds.append(pred_batch.cpu())\n                    all_targets.append(y_batch.cpu())\n    \n            all_preds = torch.cat(all_preds, dim=0)\n            all_targets = torch.cat(all_targets, dim=0)\n    \n            mrr_100 = compute_mrr_at_k_batched(all_preds.to(DEVICE), all_targets.to(DEVICE), k=100, batch_size=128)\n            model.eval()\n            print(all_preds.shape,all_targets.shape)\n            print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}, MRR@100={mrr_100:.4f}\", evaluate_retrieval(all_preds.cpu(), all_targets.cpu(), np.arange(len(all_preds))))\n            \n            if train_loss > val_loss:\n                best_mrr = mrr_100\n                patience_counter = 0\n                Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n                torch.save(model.state_dict(), model_path)\n                print(f\"  New best: {mrr_100:.4f}\")\n            else:\n                patience_counter += 1\n    \n            del all_preds, all_targets\n            torch.cuda.empty_cache()\n        else:\n            print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n    \n        if epoch < warmup_epochs:\n            scheduler_warmup.step()\n        else:\n            scheduler_cosine.step()\n    \n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(model_par):\n    print(\"\\n2. Building model...\")\n\n    model = model_par[\"CREATE_MODEL\"](model_par)\n    \n    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    return model\n\ndef create_Enc(model_par):\n    return EncoderMLP(\n        input_dim=model_par[\"INPUT_DIM\"],\n        output_dim=model_par[\"OUTPUT_DIM\"],\n        hidden_dims = model_par[\"HIDDEN_DIMS\"],\n        dropout = model_par[\"DROPOUT\"],\n    ).to(DEVICE)\n\nclass EncoderMLP(nn.Module):\n    \"\"\"MLP for projecting embeddings to shared space\"\"\"\n    \n    def __init__(self, input_dim, output_dim, hidden_dims, dropout):\n        super().__init__()\n        print(\"Creating ProjectionMLP\")\n        layers = []\n        prev_dim = input_dim\n        \n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.GELU(),\n                nn.Dropout(dropout if i < len(hidden_dims) - 1 else dropout * 0.5)\n            ])\n            prev_dim = hidden_dim\n        \n        layers.extend([\n            nn.Linear(prev_dim, output_dim),\n            nn.BatchNorm1d(output_dim)\n        ])\n        \n        self.network = nn.Sequential(*layers)\n        self.skip = nn.Linear(input_dim, output_dim)\n        self.skip_weight = nn.Parameter(torch.tensor(0.1))\n        \n    def forward(self, x):\n        out = self.network(x)\n        out = F.normalize(out, dim=-1)\n        skip = F.normalize(self.skip(x), dim=-1)\n        return out + self.skip_weight * skip\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MLP_ENC_PAR = {\"NAME\":\"ModelEncoder\", \"HIDDEN_DIMS\":[12000, 10000,8192, 4096, 2048],\"INPUT_DIM\":15724, \"OUTPUT_DIM\":768, \"DROPOUT\": 0.2, \"CREATE_MODEL\":create_Enc}\n\n#Decoder fron space 780\nENC_LOSS_PAR = {\"NAME\":\"COMB_TRIPLET_AND_CONTR\", \"FUNC\": combined_loss, \"ARG\":{\"TEMPERATURE\":0.7, \"ALPHA\":0.4, \"MARGIN\":0.5, \"BETA\":0.3}}\n\n#TRAINING\nENC_TRAINING_PAR = {\"LR\":0.0005, \"WARMUP\":5, \"EPOCHS\": 50, \"WEIGHT_DEC\":0.01, \"FLOW\":False}\n\n\n#DATASET\nENC_DATASET_PAR = {\"BATCH_SIZE\":4096*2, \"TRAIN_SIZE\":0.9}\n\n#FILES\nDATASET_PATH = \"/kaggle/input/d/niccolosici/aml-dataset/train/train.npz\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nAUGMENTATION_PAR = {\"USE_AUGMENTATION\":False}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SECTION 7: MAIN EXECUTION\n# ============================================================================\nif __name__ == \"__main__\":\n    \n    # ===== STEP 1: Download Data =====\n    betas_path, images_path, behav_path = get_dataset_paths()\n    \n    # ===== STEP 2: Create Dataset =====\n    dataset = MindEyeDataset(\n        betas_file=betas_path,\n        images_file=images_path,\n        behav_file=behav_path\n    )\n    \n    # ===== STEP 3: Create Train/Val Split =====\n    train_indices, val_indices = create_splits(dataset, train_ratio=0.8)\n    \n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    \n    # ===== STEP 4: Load CLIP =====\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    clip_extractor = CLIPEmbeddingsExtractor(device=device)\n    \n    # ===== STEP 5: Build CLIP Embeddings Cache =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"BUILDING CLIP EMBEDDINGS CACHE\")\n    print(\"=\"*60)\n    \n    clip_embeddings = build_clip_cache(\n        dataset=dataset,\n        clip_extractor=clip_extractor,\n        cache_path='clip_embeddings_subj01.pt',\n        batch_size=64\n    )\n    \n    # ===== STEP 6: Create Final Datasets =====\n    train_dataset_clip = MindEyeWithCLIP(train_dataset, clip_embeddings[train_indices])\n    val_dataset_clip = MindEyeWithCLIP(val_dataset, clip_embeddings[val_indices])\n    \n    # ===== STEP 7: Test DataLoaders =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"TESTING DATALOADERS\")\n    print(\"=\"*60)\n    \n    train_loader = DataLoader(train_dataset_clip, batch_size=32, \n                              shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset_clip, batch_size=32, \n                            shuffle=False, num_workers=0)\n\n    \n    encoder_model = create_model(MLP_ENC_PAR)\n    encoder_model = train_model(AUGMENTATION_PAR,ENC_TRAINING_PAR, ENC_LOSS_PAR, encoder_model,train_loader, val_loader, DEVICE,\"./encoderF2C.pth\")\n    del encoder_model","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}